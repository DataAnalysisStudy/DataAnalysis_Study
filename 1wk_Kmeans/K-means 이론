군집 분석 (Clustering)
	- 개체들의 특성에 따라 몇 개 의 군집으로 집단화하고, 형성된 군집들의 특성을 파악하여 군집들 사이의 관계를 분석하는 다변량분석
	- 거리를 측정하여 집단의 이질성, 동질성을 평가하여 그룹화
	- 분석 목적
		○ 집단 간 차이 분석
		○ 특이집단 포착(이상값 탐지)
		○ 군집의 결과를 분류분석의 입력 변수로 투입
		○ 유사한 값을 가진 많은 속성을 그룹화해 대량 데이터셋을 몇 개의 균일한 범주로 단순화
	- 관측 대상 간 거리
		○ 민코우스키 거리
			§ d(x,y)=(〖〖∑▒〖(x_i−y_i 〗)〗^m)〗^(1/m)
		○ 맨하탄 거리 (m=1)
			§ d(x,y)=∑▒|x_i−y_i | 
		○ 유클리드 거리 (m=2)
			§ d(x,y)=(〖〖∑▒〖(x_i−y_i 〗)〗^2)〗^(1/2)
		○ 체베셰프 거리(m=∞)
			§ d(x,y)=max|x_i−y_i |
		○ 표준화 거리
			§ d(x,y)=(〖〖∑▒〖(x_i−y_i 〗)〗^2/σ_i^2)〗^(1/2)
		○ 마할라노비스 거리
			§ d(x,y)=((〖〖x−y)〗^T S〗^(−1) (x−y))^(1/2),  S는 공분산행렬
		○ 캔버라 거리
			§ d(x,y)=∑▒|x_i−y_i |/(x_i+y_i )
		○ 명목형 변수일때 거리
			§ d(x,y)=(개체 x와 y에서 다른 값을 가지는 변수의 수)/(총변수의 수)
			§ 단순일치계수
			§ 자카드 계수
			§ 순위 상관 계수
			
	- 계층적 군집분석
		○ 군집화 순서에 따른 분류
			§ 병합 방법 - 가까운 관측치끼리 묶어 가는 방법 (주로 쓰임)
			§ 분할 방법 - 큰 군집으로부터 출발하여 군집을 분리해 나가는 방법
		○ 군집 간 거리 측정 방법에 따른 분류
			§ 최단연결법 - 두 집단 간 관측치들의 거리의 최솟값을 군집 간의 거리로 정의
			§ 최장연결법 - 두 집단 간 관측치들의 거리의 최댓값을 군집 간의 거리로 정의
			§ 평균연결법 - 두 집단 간 관측치들의 거리의 평균을 군집 간의 거리로 정의
			§ 중앙값연결법 - 두 집단 간 관측치들의 거리의 중앙값을 군집 간의 거리로 정의
			§ 중심연결법
			§ 와드연결법
		○ dendrogram - 군집결과를 나무구조로 나타내 군집 간의 구조관계를 알 수 있도록 한 그래프
			
	- 비계층적 군집분석
		○ 정해진 개수로 군집을 나누는데, 정해진 기준에 따라 더 이상 개선이 되지 않을 때까지 군집화 진행
		○ 군집화 과정에서 관측치가 다른 그룹으로 이동되기도 함
		○ 종류
			§ k-means clustering (Centroid-based clustering)
				□ 장단점
					® 장점
						◊ 통계 용어 없이 설명할 수 있게 군집 식별에 대한 간단한 원리를 사용
						◊ 매우 유연하며 간단한 수정으로 결점을 극복하게 적용할 수 있음
						◊ 효율적이고 데이터를 유용한 군집으로 나눔
					® 단점
						◊ 최근 군집화 알고리즘보다 덜 세련됨
						◊ 무작위 초기화 때문에 최적의 군집을 찾지 못할 수 있음
						◊ 데이터에서 얼마나 군집이 생성될 수 있을지 합리적인 추측이 필요
				□ 알고리즘
					® 기본 알고리즘
						◊ n개의 관측치에 대해 k개의 군집으로 나눈 경우
						◊ 임의로 k의 중심을 정해 각 관측치를 가장 가까운 중심에 묶어 첫 번째 군집을 만듦
						◊ 각 군집에 대해 군집 내 관측치의 거리를 계산하여 새로운 중심점을 구함
						◊ 새로 정한 중심점에 대해 각 관측치를 가까운 중심점으로 다시 묶어 군집을 재정의
						◊ 재정의된 군집이 이전 군집과 일치할 때까지 2,3을 반복
					® 초기값 선정 방법
						◊ Random
							} 임의의 좌표롤 선정 한 후 각 데이터들을 좌표의 클러스터에 배정한 후, 각 클러스터의 데이터의 평균 값을 초기값으로 선정
						◊ Forgy Approch (FA)
							} 임의의 데이터를 초기값으로 선정
						◊ Macqueen Approach (MA)
							} 임의의 데이터를 초기값으로 선정한 후, 초기값과 가장 가까운 데이터 순으로 클러스터에 포함시키며, 클러스터에 데이터가 추가 될 때마다 중심값을 재계산
							} 모든 계산이 종료된 중심점을 초기 값으로 선정
						◊ Kaufman Approach (KA)
							} 랜덤하게 초기값을 선택했을 때 발생할 수 있는 문제점을 해결
							} 데이터 집합 중 가장 중심에 위치한 데이터를 초기값으로 선택
				□ 군집 개수 선택
					® Rule of thumb
						◊ k=√(n/2)  ,  n=데이터 수
					® 엘보우(elbow 기법)
						◊ 여러 가지 k 값에 대해 군집 변경 내의 동질성과 이질성을 측정하는 방법
				◊ 클러스터 평가
					® 내부 평가
						◊ 특징
							} 데이터 집합/속성을 이용하여 클러스터 내 높은 유사도, 클러스터 간 낮은 유사도로 평가
							} 평가 점수가 높다고 해서 실제 참값에 가깝다는 보장을 할 수 없음
						◊ 종류
							} Davies-Bouldin Index
								– DB=1/n Σ_(i=1)^n   max_(j≠i)⁡((σ_i+σ_j)/d(c_i,c_j ) )
								– n : 클러스터 수
								– d(c_i,c_j )  : i와 j 클러스터 중심점 간 거리
								– σ_i  : i 클러스터 내의 모든 데이터에서 중심점까지의 거리 평균값
								– 값이 낮을 수록 좋은 클러스터링 알고리즘으로 평가
							} Dunn Index
								– D=min_(1≤i<j≤n)⁡〖d(i,j)〗/max_(1≤k≤n)⁡〖d^′ (k)〗 
								– n : 클러스터 수
								– d(c_i,c_j )  : i와 j 클러스터 중심점 간 거리
								– d^′ (k) : k 클러스터의 데이터끼리 거리가 가장 긴 값
								– 값이 높을수록 클러스터링 성능이 좋음
							} 실루엣 기법
								– s(i)=(b(i)−a(i))/max{a(i), b(i)} 
								– −1≤s(i)≤1
								– a(i) : 클러스터 내 데이터들 간의 거리 평균
								– b(i) : 속한 클러스터의 데이터를 제외한 모든 데이터들 간의 거리 평균
								– s(i)가 1에 가까울 수록 i는 올바른 클러스터
					® 외부 평가
						◊ 특징
							} 클러스터링에 사용되지 않은 데이터로 평가
							} 클러스터링 결과물이 미리 정해진 결과물과 얼마나 비슷한지 측정
						◊ 종류
							} Rand measure
								– RI=(TP+TN)/(TP+FP+FN+TN)
							} F-measure
								– F_β=((β^2+1)×P×R)/(β^2×P+R)
								– P=TP/(TP+FP)
								– R=TP/(TP+FN)
								– β≥를 변경하면서  재현율을 조정하여 FN 값의 비중을 변화
								– β=0일때 F_0=P => 재현율은 β=0 일 때 어떠한 영향을 미치지 못함
